{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 6: Iterative Estimation Methods\n",
    "\n",
    "## A6.1 Newton Iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(P):\n",
    "    X = none\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jacobian(P):\n",
    "    J = none\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Init():\n",
    "    P = none\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_inverse(J):\n",
    "    return (J^T*J)^(-1)*J^T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(J, e):\n",
    "    return -pseudo_inverse(J) * e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(P, X):\n",
    "    e = f(P) - X\n",
    "    J = Jacobian(P)\n",
    "    delta = solve(J, e)\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible that this iteration procedure converges to a local minimum value, or does not converge at all.\n",
    "def newton_iterate(X):\n",
    "    P = Init()\n",
    "    delta = none\n",
    "    while continue_cond(delta):\n",
    "        delta = step(P, X)\n",
    "        P += delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted iteration\n",
    "Assume the measurement X satisfies a Gaussian distribution which covariance matrix $\\Sigma_X$, and wishes to minimize the Mahalanobis distance $||f(\\hat{P}) - X||_\\Sigma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_inverse_mahalanobis(J, C):\n",
    "    return (J^T * C^(-1) * J)^(-1) * J^T * C^(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_mahalanobis(J, C, e):\n",
    "    return -pseudo_inverse_mahalanobis(J, C)* e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method and the Hessian\n",
    "\n",
    "Expanding $g(P)$ about $P_0$ in a Taylor series to get\n",
    "\n",
    "$$g(P_0 + \\Delta) = g + g_P\\Delta + \\Delta^T g_{PP}\\Delta/2 + \\dots$$\n",
    "\n",
    "where subscript P denotes differentiation, and the right hand side is evaluated at $P_0$.\n",
    "\n",
    "To minimize this quantity with respect to $\\Delta$,differentiating with respect to $\\Delta$ and setting the derivative to zero, arriving at the equation $g_P + g_{PP}\\Delta = 0$ or\n",
    "\n",
    "$$g_{PP}\\Delta = -g_P.$$\n",
    "\n",
    "$g_{PP}$ is the matrix of second derivatives, the Hessian of g, the $(i,j)$-th entry of which is $\\partial^2g/\\partial p_i\\partial p_j$, and $p_i$ and $p_j$ are the $i$-th and $j$-th parameters.\n",
    "\n",
    "Vector $g_P$ is the gradient of g.\n",
    "\n",
    "The method of *Newton iteration* consists in starting at an initial value of the parameters, $P_0$, and iteratively computing parameter increments $\\Delta$ until convergence occurs.\n",
    "\n",
    "Let g(P) is the aquared norm of an error function\n",
    "\n",
    "$$g(P) = \\frac{1}{2}||\\epsilon(P)||^2=\\frac{1}{2}\\epsilon(P)^T\\epsilon(P)$$\n",
    "\n",
    "where $\\epsilon(P)=f(P)-X$.\n",
    "\n",
    "The gradient vector $g_P$ is $\\epsilon_P^T\\epsilon$, and $\\epsilon_P=f_P=J$, so $g_P=J^T\\epsilon$.\n",
    "\n",
    "Differentiating $g_P=\\epsilon_P^T\\epsilon$ a second time to compute the formula for the Hessian.\n",
    "\n",
    "$$g_{PP}=\\epsilon_P^T\\epsilon_P+\\epsilon_{PP}^T\\epsilon$.\n",
    "\n",
    "Under the assumption that $f(P)$ is linear, the second term on the right vanishes, leaving $g_{PP}=\\epsilon_P^T\\epsilon_P=J^TJ$.\n",
    "\n",
    "This procedure in which $J^TJ$ is used as an approximation for the Hessian is known as the <mark>Gauss-Newton method</mark>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent.** A strategy for minimization of $g$ is to move iteratively in the gradient direction.\n",
    "\n",
    "The length of the step may be computed by carrying out a line search for the function minimization in the negative gradient direction. In this case, the parameter increment $\\Delta$ is computed from the equation $\\lambda\\Delta = -g_P$, where $\\lambda$ comtrols the length of the step.\n",
    "\n",
    "We may consider this as related to Newton iteration as expressed by the update equation in which the Hessian is approximated (somewhat arbitrarily) by the scalar matrix $\\lambda I$.\n",
    "\n",
    "The Levenberg-Marquardt method is essentially a Gaussian-Newton method that transitions smoothly to gradient descent when the Gauss-Newton updates fail\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three methods of minimization of a cost function $\\frac{1}{2}g(P)=||\\epsilon(P)||^2$\n",
    "\n",
    "- Newton.\n",
    "\n",
    "Update equation: $$g_{PP}\\Delta=-g_P$$\n",
    "\n",
    "where $g_{PP}=\\epsilon_P^T\\epsilon_P + \\epsilon_{PP}^T\\epsilon$ and $g_P = \\epsilon_P^T\\epsilon$.\n",
    "\n",
    "- Gauss-Newton.\n",
    "\n",
    "- Gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
